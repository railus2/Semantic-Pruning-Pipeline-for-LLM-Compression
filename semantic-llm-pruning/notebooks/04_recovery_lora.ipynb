{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":290428399,"sourceType":"kernelVersion"},{"sourceId":290434601,"sourceType":"kernelVersion"},{"sourceId":290436902,"sourceType":"kernelVersion"}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Paths IN / OUT","metadata":{}},{"cell_type":"code","source":"import os\nos.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\nos.environ[\"TRANSFORMERS_NO_FLAX\"] = \"1\"\nos.environ[\"USE_TF\"] = \"0\"\nos.environ[\"USE_FLAX\"] = \"0\"\nos.environ[\"JAX_PLATFORM_NAME\"] = \"cpu\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T00:22:22.326967Z","iopub.execute_input":"2026-01-07T00:22:22.327529Z","iopub.status.idle":"2026-01-07T00:22:22.333974Z","shell.execute_reply.started":"2026-01-07T00:22:22.327500Z","shell.execute_reply":"2026-01-07T00:22:22.333391Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Imports","metadata":{}},{"cell_type":"code","source":"!pip -q install peft accelerate datasets\n\nimport os\nimport torch\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling\nfrom peft import LoraConfig, get_peft_model\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T00:22:24.498373Z","iopub.execute_input":"2026-01-07T00:22:24.498942Z","iopub.status.idle":"2026-01-07T00:22:43.589756Z","shell.execute_reply.started":"2026-01-07T00:22:24.498911Z","shell.execute_reply":"2026-01-07T00:22:43.589152Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Fonction de chargement modèle (SAFE)","metadata":{}},{"cell_type":"code","source":"BASE_OUT = \"/kaggle/working/semantic-llm-pruning\"\n\nRECOVERY_MODELS = [\n    (\"heads20_mlp10\", \"/kaggle/input/03-structured-mlp-pruning-activation-based/semantic-llm-pruning/models/pruned_heads20_mlp10\"),\n    (\"heads20_mlp20\", \"/kaggle/input/03-structured-mlp-pruning-activation-based/semantic-llm-pruning/models/pruned_heads20_mlp20\"),\n]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T00:27:51.949706Z","iopub.execute_input":"2026-01-07T00:27:51.950578Z","iopub.status.idle":"2026-01-07T00:27:51.954530Z","shell.execute_reply.started":"2026-01-07T00:27:51.950537Z","shell.execute_reply":"2026-01-07T00:27:51.953911Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"MODEL_MAX_LEN = 512\n\ndef format_example(ex):\n    # Alpaca format standard\n    instr = ex.get(\"instruction\", \"\")\n    inp = ex.get(\"input\", \"\")\n    out = ex.get(\"output\", \"\")\n    prompt = f\"### Instruction:\\n{instr}\\n\"\n    if inp and isinstance(inp, str) and inp.strip():\n        prompt += f\"### Input:\\n{inp}\\n\"\n    prompt += f\"### Response:\\n{out}\\n\"\n    return {\"text\": prompt}\n\nds = load_dataset(\"tatsu-lab/alpaca\", split=\"train\")\nds = ds.shuffle(seed=42).select(range(5000))  # petit subset Kaggle\nds = ds.map(format_example, remove_columns=ds.column_names)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T00:27:55.958129Z","iopub.execute_input":"2026-01-07T00:27:55.958893Z","iopub.status.idle":"2026-01-07T00:27:56.430447Z","shell.execute_reply.started":"2026-01-07T00:27:55.958863Z","shell.execute_reply":"2026-01-07T00:27:56.429721Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def run_lora_recovery(tag, model_path):\n    tok = AutoTokenizer.from_pretrained(model_path, use_fast=True)\n    if tok.pad_token is None:\n        tok.pad_token = tok.eos_token\n\n    def tokenize_fn(ex):\n        enc = tok(\n            ex[\"text\"],\n            truncation=True,\n            max_length=MODEL_MAX_LEN,\n            padding=\"max_length\",\n        )\n        enc[\"labels\"] = enc[\"input_ids\"].copy()\n        return enc\n\n    tokenized = ds.map(tokenize_fn, batched=False, remove_columns=[\"text\"])\n\n    model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16).to(\"cuda\")\n    model.train()\n\n    # LoRA sur attention + MLP (classique et efficace)\n    lora_cfg = LoraConfig(\n        r=8,\n        lora_alpha=16,\n        lora_dropout=0.05,\n        bias=\"none\",\n        task_type=\"CAUSAL_LM\",\n        target_modules=[\n            \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\n            \"gate_proj\",\"up_proj\",\"down_proj\"\n        ],\n    )\n    model = get_peft_model(model, lora_cfg)\n    model.print_trainable_parameters()\n\n    args = TrainingArguments(\n        output_dir=f\"{BASE_OUT}/results/lora_recovery/{tag}\",\n        num_train_epochs=1,\n        per_device_train_batch_size=1,\n        gradient_accumulation_steps=16,\n        learning_rate=2e-4,\n        fp16=True,\n        logging_steps=20,\n        save_steps=200,\n        save_total_limit=2,\n        report_to=\"none\",\n    )\n\n    collator = DataCollatorForLanguageModeling(tok, mlm=False)\n\n    trainer = Trainer(\n        model=model,\n        args=args,\n        train_dataset=tokenized,\n        data_collator=collator,\n    )\n\n    trainer.train()\n\n    # Sauver l’adapter LoRA (léger)\n    out_adapter = f\"{BASE_OUT}/models/{tag}_lora_adapter\"\n    os.makedirs(out_adapter, exist_ok=True)\n    trainer.model.save_pretrained(out_adapter)\n    tok.save_pretrained(out_adapter)\n    print(\"Saved LoRA adapter ->\", out_adapter)\n\n    # Option: sauver le modèle mergé (plus lourd, mais pratique pour eval)\n    merged = trainer.model.merge_and_unload()\n    out_merged = f\"{BASE_OUT}/models/{tag}_lora_merged\"\n    os.makedirs(out_merged, exist_ok=True)\n    merged.save_pretrained(out_merged)\n    tok.save_pretrained(out_merged)\n    print(\"Saved merged model ->\", out_merged)\n\n    return out_adapter, out_merged\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T00:28:00.574800Z","iopub.execute_input":"2026-01-07T00:28:00.575127Z","iopub.status.idle":"2026-01-07T00:28:00.583649Z","shell.execute_reply.started":"2026-01-07T00:28:00.575074Z","shell.execute_reply":"2026-01-07T00:28:00.582910Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"saved = []\nfor tag, path in RECOVERY_MODELS:\n    saved.append((tag, *run_lora_recovery(tag, path)))\n\nsaved\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T00:28:04.194327Z","iopub.execute_input":"2026-01-07T00:28:04.195121Z","iopub.status.idle":"2026-01-07T01:29:39.460894Z","shell.execute_reply.started":"2026-01-07T00:28:04.195063Z","shell.execute_reply":"2026-01-07T01:29:39.460179Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def quick_generate(model_path, prompt=\"Give 2 bullet points about pruning.\", max_new_tokens=80):\n    tok = AutoTokenizer.from_pretrained(model_path, use_fast=True)\n    if tok.pad_token is None:\n        tok.pad_token = tok.eos_token\n    m = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16).to(\"cuda\").eval()\n    inputs = tok(prompt, return_tensors=\"pt\").to(\"cuda\")\n    out = m.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)\n    print(tok.decode(out[0], skip_special_tokens=True))\n\nfor tag, adapter, merged in saved:\n    print(\"\\n====\", tag, \"merged ====\")\n    quick_generate(merged)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T01:29:43.090737Z","iopub.execute_input":"2026-01-07T01:29:43.091309Z","iopub.status.idle":"2026-01-07T01:29:50.453361Z","shell.execute_reply.started":"2026-01-07T01:29:43.091268Z","shell.execute_reply":"2026-01-07T01:29:50.452601Z"}},"outputs":[],"execution_count":null}]}