{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":290428399,"sourceType":"kernelVersion"},{"sourceId":290434601,"sourceType":"kernelVersion"},{"sourceId":290436902,"sourceType":"kernelVersion"}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Paths IN / OUT","metadata":{}},{"cell_type":"code","source":"import os\nos.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\nos.environ[\"TRANSFORMERS_NO_FLAX\"] = \"1\"\nos.environ[\"USE_TF\"] = \"0\"\nos.environ[\"USE_FLAX\"] = \"0\"\nos.environ[\"JAX_PLATFORM_NAME\"] = \"cpu\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T11:58:44.543355Z","iopub.execute_input":"2026-01-07T11:58:44.543682Z","iopub.status.idle":"2026-01-07T11:58:44.550705Z","shell.execute_reply.started":"2026-01-07T11:58:44.543655Z","shell.execute_reply":"2026-01-07T11:58:44.549994Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"Imports","metadata":{}},{"cell_type":"code","source":"!pip -q install peft accelerate datasets\n\nimport os\nimport torch\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling\nfrom peft import LoraConfig, get_peft_model\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T11:58:44.552564Z","iopub.execute_input":"2026-01-07T11:58:44.553086Z","iopub.status.idle":"2026-01-07T11:59:03.299057Z","shell.execute_reply.started":"2026-01-07T11:58:44.553051Z","shell.execute_reply":"2026-01-07T11:59:03.298433Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"Fonction de chargement modèle (SAFE)","metadata":{}},{"cell_type":"code","source":"BASE_OUT = \"/kaggle/working/semantic-llm-pruning\"\n\nRECOVERY_MODELS = [\n    (\"heads20_mlp10\", \"/kaggle/input/03-structured-mlp-pruning-activation-based/semantic-llm-pruning/models/pruned_heads20_mlp10\"),\n    (\"heads20_mlp20\", \"/kaggle/input/03-structured-mlp-pruning-activation-based/semantic-llm-pruning/models/pruned_heads20_mlp20\"),\n]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T11:59:03.300663Z","iopub.execute_input":"2026-01-07T11:59:03.301356Z","iopub.status.idle":"2026-01-07T11:59:03.305200Z","shell.execute_reply.started":"2026-01-07T11:59:03.301325Z","shell.execute_reply":"2026-01-07T11:59:03.304339Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"MODEL_MAX_LEN = 512\n\ndef format_example(ex):\n    # Alpaca format standard\n    instr = ex.get(\"instruction\", \"\")\n    inp = ex.get(\"input\", \"\")\n    out = ex.get(\"output\", \"\")\n    prompt = f\"### Instruction:\\n{instr}\\n\"\n    if inp and isinstance(inp, str) and inp.strip():\n        prompt += f\"### Input:\\n{inp}\\n\"\n    prompt += f\"### Response:\\n{out}\\n\"\n    return {\"text\": prompt}\n\nds = load_dataset(\"tatsu-lab/alpaca\", split=\"train\")\nds = ds.shuffle(seed=42).select(range(5000))  # petit subset Kaggle\nds = ds.map(format_example, remove_columns=ds.column_names)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T11:59:03.306286Z","iopub.execute_input":"2026-01-07T11:59:03.306533Z","iopub.status.idle":"2026-01-07T11:59:05.981493Z","shell.execute_reply.started":"2026-01-07T11:59:03.306509Z","shell.execute_reply":"2026-01-07T11:59:05.980643Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd539277073344478c58fc8c82e953c3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/train-00000-of-00001-a09b74b3ef9c3b(…):   0%|          | 0.00/24.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"148fd8022ac444ed830f3cd8d365d886"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/52002 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e3c9b22e17d4024b197e6fe065f0004"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"475644cc431541c4abf38086783d9f70"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"def run_lora_recovery(tag, model_path):\n    tok = AutoTokenizer.from_pretrained(model_path, use_fast=True)\n    if tok.pad_token is None:\n        tok.pad_token = tok.eos_token\n\n    def tokenize_fn(ex):\n        enc = tok(\n            ex[\"text\"],\n            truncation=True,\n            max_length=MODEL_MAX_LEN,\n            padding=\"max_length\",\n        )\n        enc[\"labels\"] = enc[\"input_ids\"].copy()\n        return enc\n\n    tokenized = ds.map(tokenize_fn, batched=False, remove_columns=[\"text\"])\n\n    model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16).to(\"cuda\")\n    model.train()\n\n    # LoRA sur attention + MLP (classique et efficace)\n    lora_cfg = LoraConfig(\n        r=8,\n        lora_alpha=16,\n        lora_dropout=0.05,\n        bias=\"none\",\n        task_type=\"CAUSAL_LM\",\n        target_modules=[\n            \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\n            \"gate_proj\",\"up_proj\",\"down_proj\"\n        ],\n    )\n    model = get_peft_model(model, lora_cfg)\n    model.print_trainable_parameters()\n\n    args = TrainingArguments(\n        output_dir=f\"{BASE_OUT}/results/lora_recovery/{tag}\",\n        num_train_epochs=1,\n        per_device_train_batch_size=1,\n        gradient_accumulation_steps=16,\n        learning_rate=2e-4,\n        fp16=True,\n        logging_steps=20,\n        save_steps=200,\n        save_total_limit=2,\n        report_to=\"none\",\n    )\n\n    collator = DataCollatorForLanguageModeling(tok, mlm=False)\n\n    trainer = Trainer(\n        model=model,\n        args=args,\n        train_dataset=tokenized,\n        data_collator=collator,\n    )\n\n    trainer.train()\n\n    # Sauver l’adapter LoRA (léger)\n    out_adapter = f\"{BASE_OUT}/models/{tag}_lora_adapter\"\n    os.makedirs(out_adapter, exist_ok=True)\n    trainer.model.save_pretrained(out_adapter)\n    tok.save_pretrained(out_adapter)\n    print(\"Saved LoRA adapter ->\", out_adapter)\n\n    # Option: sauver le modèle mergé (plus lourd, mais pratique pour eval)\n    merged = trainer.model.merge_and_unload()\n    out_merged = f\"{BASE_OUT}/models/{tag}_lora_merged\"\n    os.makedirs(out_merged, exist_ok=True)\n    merged.save_pretrained(out_merged)\n    tok.save_pretrained(out_merged)\n    print(\"Saved merged model ->\", out_merged)\n\n    return out_adapter, out_merged\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T11:59:05.982610Z","iopub.execute_input":"2026-01-07T11:59:05.983024Z","iopub.status.idle":"2026-01-07T11:59:05.991620Z","shell.execute_reply.started":"2026-01-07T11:59:05.982996Z","shell.execute_reply":"2026-01-07T11:59:05.990745Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"saved = []\nfor tag, path in RECOVERY_MODELS:\n    saved.append((tag, *run_lora_recovery(tag, path)))\n\nsaved\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T11:59:05.993303Z","iopub.execute_input":"2026-01-07T11:59:05.993511Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b369d2cbb254c20987667ce0df0654c"}},"metadata":{}},{"name":"stderr","text":"`torch_dtype` is deprecated! Use `dtype` instead!\n","output_type":"stream"},{"name":"stdout","text":"trainable params: 5,807,296 || all params: 977,716,416 || trainable%: 0.5940\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='157' max='157' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [157/157 30:26, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>20</td>\n      <td>1.796400</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>1.500800</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>1.480400</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>1.451900</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>1.422400</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>1.459000</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>1.403500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Saved LoRA adapter -> /kaggle/working/semantic-llm-pruning/models/heads20_mlp10_lora_adapter\nSaved merged model -> /kaggle/working/semantic-llm-pruning/models/heads20_mlp10_lora_merged\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03f9afc5567849f1ae5d244517853787"}},"metadata":{}},{"name":"stdout","text":"trainable params: 5,510,032 || all params: 901,319,568 || trainable%: 0.6113\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='135' max='157' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [135/157 25:36 < 04:14, 0.09 it/s, Epoch 0.86/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>20</td>\n      <td>1.967700</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>1.626300</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>1.597000</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>1.561000</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>1.527400</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>1.557700</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"def quick_generate(model_path, prompt=\"Give 2 bullet points about pruning.\", max_new_tokens=80):\n    tok = AutoTokenizer.from_pretrained(model_path, use_fast=True)\n    if tok.pad_token is None:\n        tok.pad_token = tok.eos_token\n    m = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16).to(\"cuda\").eval()\n    inputs = tok(prompt, return_tensors=\"pt\").to(\"cuda\")\n    out = m.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)\n    print(tok.decode(out[0], skip_special_tokens=True))\n\nfor tag, adapter, merged in saved:\n    print(\"\\n====\", tag, \"merged ====\")\n    quick_generate(merged)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}