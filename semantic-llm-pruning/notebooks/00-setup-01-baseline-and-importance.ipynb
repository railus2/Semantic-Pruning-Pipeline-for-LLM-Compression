{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14414164,"sourceType":"datasetVersion","datasetId":9206187}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\n\nBASE = \"/kaggle/working/semantic-llm-pruning\"\nos.makedirs(f\"{BASE}/results/ablation_studies\", exist_ok=True)\nos.makedirs(f\"{BASE}/models\", exist_ok=True)\nos.makedirs(f\"{BASE}/artifacts\", exist_ok=True)  # optionnel\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-07T11:20:17.772207Z","iopub.execute_input":"2026-01-07T11:20:17.772388Z","iopub.status.idle":"2026-01-07T11:20:17.780709Z","shell.execute_reply.started":"2026-01-07T11:20:17.772368Z","shell.execute_reply":"2026-01-07T11:20:17.779925Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!pip -q install -U \"transformers>=4.44\" \"accelerate>=0.33\" \"datasets>=2.20\" einops peft evaluate sentencepiece","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T11:20:17.782016Z","iopub.execute_input":"2026-01-07T11:20:17.782351Z","iopub.status.idle":"2026-01-07T11:20:35.168498Z","shell.execute_reply.started":"2026-01-07T11:20:17.782319Z","shell.execute_reply":"2026-01-07T11:20:35.167782Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m85.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m380.9/380.9 kB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m512.3/512.3 kB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m556.4/556.4 kB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nBASE = \"/kaggle/working/semantic-llm-pruning\"\nos.environ[\"HF_HOME\"] = f\"{BASE}/.hf\"\nos.environ[\"TRANSFORMERS_CACHE\"] = f\"{BASE}/.hf/transformers\"\nos.environ[\"HF_DATASETS_CACHE\"] = f\"{BASE}/.hf/datasets\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T11:20:35.169562Z","iopub.execute_input":"2026-01-07T11:20:35.169809Z","iopub.status.idle":"2026-01-07T11:20:35.173914Z","shell.execute_reply.started":"2026-01-07T11:20:35.169784Z","shell.execute_reply":"2026-01-07T11:20:35.173328Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import os\n\n# Empêche transformers d'aller chercher TF/JAX/Flax (cause de ton crash)\nos.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\nos.environ[\"TRANSFORMERS_NO_FLAX\"] = \"1\"\nos.environ[\"USE_TF\"] = \"0\"\nos.environ[\"USE_FLAX\"] = \"0\"\nos.environ[\"JAX_PLATFORM_NAME\"] = \"cpu\"\n\n# Cache HF (suffisant, pas besoin de TRANSFORMERS_CACHE)\nBASE = \"/kaggle/working/semantic-llm-pruning\"\nos.environ[\"HF_HOME\"] = f\"{BASE}/.hf\"\nos.environ[\"HF_DATASETS_CACHE\"] = f\"{BASE}/.hf/datasets\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T11:20:35.174885Z","iopub.execute_input":"2026-01-07T11:20:35.175154Z","iopub.status.idle":"2026-01-07T11:20:35.188133Z","shell.execute_reply.started":"2026-01-07T11:20:35.175123Z","shell.execute_reply":"2026-01-07T11:20:35.187451Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import torch\nimport transformers\nimport datasets\nimport accelerate\n\nprint(\"CUDA:\", torch.cuda.is_available(), torch.cuda.get_device_name(0) if torch.cuda.is_available() else None)\nprint(\"transformers:\", transformers.__version__)\nprint(\"datasets:\", datasets.__version__)\nprint(\"accelerate:\", accelerate.__version__)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T11:20:35.190191Z","iopub.execute_input":"2026-01-07T11:20:35.190606Z","iopub.status.idle":"2026-01-07T11:20:42.330706Z","shell.execute_reply.started":"2026-01-07T11:20:35.190578Z","shell.execute_reply":"2026-01-07T11:20:42.329911Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"CUDA: True Tesla T4\ntransformers: 4.57.3\ndatasets: 4.4.2\naccelerate: 1.12.0\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\nMODEL_ID = \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\"\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_ID,\n    dtype=torch.float16,\n    device_map=\"auto\",\n    attn_implementation=\"eager\",   # <-- IMPORTANT\n)\nmodel.eval()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T11:20:42.334187Z","iopub.execute_input":"2026-01-07T11:20:42.334725Z","iopub.status.idle":"2026-01-07T11:21:02.346276Z","shell.execute_reply.started":"2026-01-07T11:20:42.334700Z","shell.execute_reply":"2026-01-07T11:21:02.345676Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/776 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"210b330119b847dd93f3ed9402c3433e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e1add83e26aa49b2bf03e4ee84ddfca4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52b9254ba16542cfbcefb36643e736b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54720dc74f40433a9ac82a4791f19f1a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/560 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b28e9c329bf4799a9b3936c63685f9d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/4.40G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30a8c1d7b4164842b92eb228a80467b8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/129 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"366571d8c45c4c47bbf06967abeccda7"}},"metadata":{}},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"LlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(32000, 2048)\n    (layers): ModuleList(\n      (0-21): 22 x LlamaDecoderLayer(\n        (self_attn): LlamaAttention(\n          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n          (act_fn): SiLUActivation()\n        )\n        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n      )\n    )\n    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n    (rotary_emb): LlamaRotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n)"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"import math, json, time\nfrom datasets import load_dataset\n\ndef perplexity_wikitext2(model, tokenizer, max_samples=256, seq_len=1024, stride=512):\n    ds = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n    # concat simple\n    text = \"\\n\\n\".join(ds[\"text\"])\n    enc = tokenizer(text, return_tensors=\"pt\")\n    input_ids = enc[\"input_ids\"][0]\n\n    device = next(model.parameters()).device\n    nlls = []\n    n_tokens = 0\n\n    t0 = time.time()\n    model.eval()\n    with torch.no_grad():\n        # sliding window\n        for start in range(0, min(len(input_ids) - 2, max_samples * stride), stride):\n            end = min(start + seq_len, len(input_ids))\n            trg_len = end - start\n            if trg_len < 2:\n                break\n\n            x = input_ids[start:end].unsqueeze(0).to(device)\n            # labels = x, mais on masque le prefix si stride < seq_len\n            labels = x.clone()\n            if start > 0:\n                # on ne compte que la nouvelle partie\n                labels[:, :-min(stride, labels.size(1))] = -100\n\n            out = model(input_ids=x, labels=labels)\n            neg_log_likelihood = out.loss * (labels != -100).sum()\n\n            nlls.append(neg_log_likelihood.detach().float().cpu())\n            n_tokens += (labels != -100).sum().item()\n\n    ppl = torch.exp(torch.stack(nlls).sum() / max(n_tokens, 1)).item()\n    return ppl, time.time() - t0\n\nppl, elapsed = perplexity_wikitext2(model, tokenizer, max_samples=256, seq_len=1024, stride=512)\nprint(f\"Perplexity (WikiText-2 test) = {ppl:.3f} | time={elapsed:.1f}s\")\n\n# métriques hardware simples\ngpu_name = torch.cuda.get_device_name(0) if torch.cuda.is_available() else None\nmax_mem = torch.cuda.max_memory_allocated() / (1024**3) if torch.cuda.is_available() else None\n\nbaseline = {\n    \"model_id\": MODEL_ID,\n    \"dataset\": \"wikitext-2-raw-v1:test\",\n    \"metric\": \"perplexity\",\n    \"perplexity\": ppl,\n    \"eval_seconds\": elapsed,\n    \"gpu\": gpu_name,\n    \"max_memory_allocated_gb\": max_mem,\n    \"dtype\": \"float16\",\n}\n\nout_path = f\"{BASE}/results/perplexity_baseline.json\"\nwith open(out_path, \"w\") as f:\n    json.dump(baseline, f, indent=2)\n\nprint(\"Saved:\", out_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T11:21:02.347286Z","iopub.execute_input":"2026-01-07T11:21:02.348042Z","iopub.status.idle":"2026-01-07T11:22:01.277102Z","shell.execute_reply.started":"2026-01-07T11:21:02.348014Z","shell.execute_reply":"2026-01-07T11:22:01.276336Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98a4ef33908c4998938b05e8a7f4fd4e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"wikitext-2-raw-v1/test-00000-of-00001.pa(…):   0%|          | 0.00/733k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d4469e0686e4a03be398e9b83f1769a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"wikitext-2-raw-v1/train-00000-of-00001.p(…):   0%|          | 0.00/6.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"391c0f1e6b804e80b64b52310701d65f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"wikitext-2-raw-v1/validation-00000-of-00(…):   0%|          | 0.00/657k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"946045ddb0cd498c8b659d2f4dfe7704"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b7c586c6fe0a4cad9b2a4c889679c7f7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49a679276ba749c68a261b3e65380420"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e95e506969ce4bcfacf55ae464302204"}},"metadata":{}},{"name":"stdout","text":"Perplexity (WikiText-2 test) = 7.382 | time=52.7s\nSaved: /kaggle/working/semantic-llm-pruning/results/perplexity_baseline.json\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import time\nimport torch\n\n@torch.no_grad()\ndef measure_latency_ms_per_token(model, tokenizer, prompt=\"Write a short paragraph about pruning.\", gen_tokens=64, warmup=1, runs=3):\n    device = next(model.parameters()).device\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n\n    # warmup\n    for _ in range(warmup):\n        _ = model.generate(**inputs, max_new_tokens=gen_tokens, do_sample=False)\n\n    torch.cuda.synchronize() if torch.cuda.is_available() else None\n\n    times = []\n    for _ in range(runs):\n        t0 = time.time()\n        out = model.generate(**inputs, max_new_tokens=gen_tokens, do_sample=False)\n        torch.cuda.synchronize() if torch.cuda.is_available() else None\n        dt = time.time() - t0\n        times.append(dt)\n\n    avg = sum(times)/len(times)\n    ms_per_token = (avg * 1000) / gen_tokens\n    return ms_per_token\n\nms_tok = measure_latency_ms_per_token(model, tokenizer)\nprint(f\"Latency ≈ {ms_tok:.2f} ms/token\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T11:22:01.278072Z","iopub.execute_input":"2026-01-07T11:22:01.278364Z","iopub.status.idle":"2026-01-07T11:22:11.136795Z","shell.execute_reply.started":"2026-01-07T11:22:01.278337Z","shell.execute_reply":"2026-01-07T11:22:11.135988Z"}},"outputs":[{"name":"stdout","text":"Latency ≈ 37.81 ms/token\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import os, json\nimport numpy as np\nimport torch\nfrom datasets import load_dataset\n\nBASE = \"/kaggle/working/semantic-llm-pruning\"\nART = f\"{BASE}/artifacts\"\nos.makedirs(ART, exist_ok=True)\n\nds = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n\ndef make_batches(tokenizer, ds, batch_size=1, seq_len=256, max_batches=200):\n    # concat léger + chunking\n    text = \"\\n\\n\".join(ds[\"text\"][:5000])\n    ids = tokenizer(text, return_tensors=\"pt\")[\"input_ids\"][0]\n    # découpe en chunks seq_len\n    chunks = []\n    for i in range(0, min(len(ids) - seq_len - 1, max_batches * seq_len), seq_len):\n        chunks.append(ids[i:i+seq_len])\n        if len(chunks) >= batch_size * max_batches:\n            break\n\n    # yield batches\n    for i in range(0, len(chunks), batch_size):\n        b = chunks[i:i+batch_size]\n        if len(b) < batch_size:\n            break\n        yield torch.stack(b, dim=0)\n\nbatches = list(make_batches(tokenizer, ds, batch_size=1, seq_len=256, max_batches=200))\nlen(batches), batches[0].shape\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T11:22:11.137715Z","iopub.execute_input":"2026-01-07T11:22:11.138002Z","iopub.status.idle":"2026-01-07T11:22:13.247276Z","shell.execute_reply.started":"2026-01-07T11:22:11.137977Z","shell.execute_reply":"2026-01-07T11:22:13.246478Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"(200, torch.Size([1, 256]))"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"x0 = batches[0].to(next(model.parameters()).device)\nout0 = model(input_ids=x0, output_attentions=True, return_dict=True, use_cache=False)\nprint(out0.attentions is not None, len(out0.attentions) if out0.attentions is not None else None)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T11:22:13.248242Z","iopub.execute_input":"2026-01-07T11:22:13.248493Z","iopub.status.idle":"2026-01-07T11:22:13.325768Z","shell.execute_reply.started":"2026-01-07T11:22:13.248470Z","shell.execute_reply":"2026-01-07T11:22:13.324929Z"}},"outputs":[{"name":"stdout","text":"True 22\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"import json\nimport numpy as np\nimport torch\n\ndevice = next(model.parameters()).device\nn_layers = model.config.num_hidden_layers\nn_heads  = model.config.num_attention_heads\n\nhead_entropy_sum = torch.zeros((n_layers, n_heads), dtype=torch.float64)\nhead_count = 0\n\n@torch.no_grad()\ndef attention_entropy(attn):  # attn: [B, H, T, S]\n    eps = 1e-9\n    p = attn.clamp_min(eps)\n    ent = -(p * p.log()).sum(dim=-1)   # [B, H, T]\n    return ent.mean(dim=(0, 2))        # [H]\n\nmodel.eval()\nfor x in batches:\n    x = x.to(device)\n    out = model(input_ids=x, output_attentions=True, return_dict=True, use_cache=False)\n    # out.attentions: tuple length L, each [B,H,T,S]\n    for l, attn in enumerate(out.attentions):\n        head_entropy_sum[l] += attention_entropy(attn).double().cpu()\n    head_count += 1\n\nhead_entropy = (head_entropy_sum / max(head_count, 1)).numpy()  # [L,H]\n\n# ranking: plus faible entropie = plus \"focus\" (souvent + important)\nranking = [{\"layer\": l, \"head\": h, \"entropy\": float(head_entropy[l, h])}\n           for l in range(n_layers) for h in range(n_heads)]\nranking_sorted = sorted(ranking, key=lambda d: d[\"entropy\"])\n\nnp.save(f\"{ART}/head_importance.npy\", head_entropy)\nwith open(f\"{ART}/head_ranking.json\", \"w\") as f:\n    json.dump({\n        \"metric\": \"attention_entropy_mean\",\n        \"lower_is_more_important\": True,\n        \"num_layers\": n_layers,\n        \"num_heads\": n_heads,\n        \"num_batches\": head_count,\n        \"seq_len\": int(batches[0].shape[1]),\n        \"ranking\": ranking_sorted\n    }, f, indent=2)\n\nprint(\"Saved:\", f\"{ART}/head_importance.npy\")\nprint(\"Saved:\", f\"{ART}/head_ranking.json\")\nprint(\"Top-10 heads (lowest entropy):\")\nprint(ranking_sorted[:10])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T11:22:13.326788Z","iopub.execute_input":"2026-01-07T11:22:13.327333Z","iopub.status.idle":"2026-01-07T11:22:25.538792Z","shell.execute_reply.started":"2026-01-07T11:22:13.327288Z","shell.execute_reply":"2026-01-07T11:22:25.538154Z"}},"outputs":[{"name":"stdout","text":"Saved: /kaggle/working/semantic-llm-pruning/artifacts/head_importance.npy\nSaved: /kaggle/working/semantic-llm-pruning/artifacts/head_ranking.json\nTop-10 heads (lowest entropy):\n[{'layer': 0, 'head': 0, 'entropy': nan}, {'layer': 0, 'head': 1, 'entropy': nan}, {'layer': 0, 'head': 2, 'entropy': nan}, {'layer': 0, 'head': 3, 'entropy': nan}, {'layer': 0, 'head': 4, 'entropy': nan}, {'layer': 0, 'head': 5, 'entropy': nan}, {'layer': 0, 'head': 6, 'entropy': nan}, {'layer': 0, 'head': 7, 'entropy': nan}, {'layer': 0, 'head': 8, 'entropy': nan}, {'layer': 0, 'head': 9, 'entropy': nan}]\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import json\nimport numpy as np\nimport torch\n\ndevice = next(model.parameters()).device\nn_layers = model.config.num_hidden_layers\n\nstate = {\n    \"mlp_sum\": None,    # tensor [L, intermediate_size]\n    \"count\": 0\n}\n\nhandles = []\n\ndef make_mlp_hook(layer_idx):\n    def hook(module, inputs, output):\n        # inputs[0]: [B, T, intermediate_size]\n        x = inputs[0].detach()\n        s = x.abs().mean(dim=(0, 1)).cpu()  # [intermediate_size]\n\n        if state[\"mlp_sum\"] is None:\n            state[\"mlp_sum\"] = torch.zeros(\n                (n_layers, s.numel()),\n                dtype=torch.float64\n            )\n\n        state[\"mlp_sum\"][layer_idx] += s.double()\n    return hook\n\n# Register hooks\nfor l in range(n_layers):\n    down_proj = model.model.layers[l].mlp.down_proj\n    handles.append(down_proj.register_forward_hook(make_mlp_hook(l)))\n\nmodel.eval()\nwith torch.no_grad():\n    for x in batches:\n        x = x.to(device)\n        _ = model(input_ids=x, return_dict=True, use_cache=False)\n        state[\"count\"] += 1\n\n# Remove hooks\nfor h in handles:\n    h.remove()\n\nmlp_importance = (state[\"mlp_sum\"] / max(state[\"count\"], 1)).numpy()  # [L, intermediate_size]\n\nranking = [\n    {\"layer\": l, \"neuron\": i, \"mean_abs_act\": float(mlp_importance[l, i])}\n    for l in range(n_layers)\n    for i in range(mlp_importance.shape[1])\n]\nranking_sorted = sorted(ranking, key=lambda d: d[\"mean_abs_act\"], reverse=True)\n\nnp.save(f\"{ART}/mlp_importance.npy\", mlp_importance)\nwith open(f\"{ART}/mlp_ranking.json\", \"w\") as f:\n    json.dump({\n        \"metric\": \"mean_abs_activation_before_down_proj\",\n        \"higher_is_more_important\": True,\n        \"num_layers\": n_layers,\n        \"intermediate_size\": int(mlp_importance.shape[1]),\n        \"num_batches\": state[\"count\"],\n        \"seq_len\": int(batches[0].shape[1]),\n        \"ranking\": ranking_sorted[:2000]\n    }, f, indent=2)\n\nprint(\"Saved:\", f\"{ART}/mlp_importance.npy\")\nprint(\"Saved:\", f\"{ART}/mlp_ranking.json\")\nprint(\"Top-10 neurons:\", ranking_sorted[:10])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T11:22:25.539617Z","iopub.execute_input":"2026-01-07T11:22:25.539877Z","iopub.status.idle":"2026-01-07T11:22:39.016895Z","shell.execute_reply.started":"2026-01-07T11:22:25.539842Z","shell.execute_reply":"2026-01-07T11:22:39.016112Z"}},"outputs":[{"name":"stdout","text":"Saved: /kaggle/working/semantic-llm-pruning/artifacts/mlp_importance.npy\nSaved: /kaggle/working/semantic-llm-pruning/artifacts/mlp_ranking.json\nTop-10 neurons: [{'layer': 20, 'neuron': 3279, 'mean_abs_act': 3.2506298828125}, {'layer': 21, 'neuron': 4696, 'mean_abs_act': 2.2972216796875}, {'layer': 21, 'neuron': 1940, 'mean_abs_act': 2.2022802734375}, {'layer': 21, 'neuron': 4132, 'mean_abs_act': 2.0382861328125}, {'layer': 21, 'neuron': 5002, 'mean_abs_act': 1.92822998046875}, {'layer': 21, 'neuron': 1955, 'mean_abs_act': 1.882861328125}, {'layer': 21, 'neuron': 3171, 'mean_abs_act': 1.756611328125}, {'layer': 21, 'neuron': 596, 'mean_abs_act': 1.7434521484375}, {'layer': 21, 'neuron': 3053, 'mean_abs_act': 1.7325732421875}, {'layer': 21, 'neuron': 3201, 'mean_abs_act': 1.627783203125}]\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"print(mlp_importance.shape)   # (22, intermediate_size)\nprint(\"Mean score:\", mlp_importance.mean())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T11:22:39.019359Z","iopub.execute_input":"2026-01-07T11:22:39.019637Z","iopub.status.idle":"2026-01-07T11:22:39.024019Z","shell.execute_reply.started":"2026-01-07T11:22:39.019614Z","shell.execute_reply":"2026-01-07T11:22:39.023461Z"}},"outputs":[{"name":"stdout","text":"(22, 5632)\nMean score: 0.031226783112684366\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"import json, time\nimport torch\n\nBASE = \"/kaggle/working/semantic-llm-pruning\"\n\nlatency = {\n    \"model_id\": MODEL_ID,\n    \"metric\": \"ms_per_token\",\n    \"ms_per_token\": float(ms_tok),\n    \"prompt\": \"Write a short paragraph about pruning.\",\n    \"gen_tokens\": 64,\n    \"gpu\": torch.cuda.get_device_name(0) if torch.cuda.is_available() else None,\n    \"dtype\": str(next(model.parameters()).dtype).replace(\"torch.\", \"\"),\n    \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n}\n\nout_path = f\"{BASE}/results/latency_baseline.json\"\nwith open(out_path, \"w\") as f:\n    json.dump(latency, f, indent=2)\n\nprint(\"Saved:\", out_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T11:22:39.024871Z","iopub.execute_input":"2026-01-07T11:22:39.025151Z","iopub.status.idle":"2026-01-07T11:22:39.038525Z","shell.execute_reply.started":"2026-01-07T11:22:39.025119Z","shell.execute_reply":"2026-01-07T11:22:39.037854Z"}},"outputs":[{"name":"stdout","text":"Saved: /kaggle/working/semantic-llm-pruning/results/latency_baseline.json\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"import os, time\nimport pandas as pd\nimport torch\n\nBASE = \"/kaggle/working/semantic-llm-pruning\"\nOUT_CSV = f\"{BASE}/results/ablation_studies/baseline.csv\"\nos.makedirs(os.path.dirname(OUT_CSV), exist_ok=True)\n\ndef count_params(m):\n    return sum(p.numel() for p in m.parameters())\n\nrow = {\n    \"model\": str(MODEL_ID),\n    \"input_model\": \"baseline\",\n    \"tag\": \"baseline\",\n    \"prune_ratio_heads\": 0.0,\n    \"prune_ratio_mlp\": 0.0,\n    \"ppl\": float(ppl),          # <-- variable EXISTANTE\n    \"latency_ms_per_token\": float(ms_tok),\n    \"params\": int(count_params(model)),\n    \"gpu\": torch.cuda.get_device_name(0) if torch.cuda.is_available() else None,\n    \"dtype\": str(next(model.parameters()).dtype).replace(\"torch.\", \"\"),\n    \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n}\n\ndf_new = pd.DataFrame([row])\n\nif os.path.exists(OUT_CSV):\n    df_old = pd.read_csv(OUT_CSV)\n    df = pd.concat([df_old, df_new], ignore_index=True)\nelse:\n    df = df_new\n\ndf.to_csv(OUT_CSV, index=False)\nprint(\"Saved baseline CSV ->\", OUT_CSV)\nprint(df.tail(1))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T11:22:39.039379Z","iopub.execute_input":"2026-01-07T11:22:39.040122Z","iopub.status.idle":"2026-01-07T11:22:39.077707Z","shell.execute_reply.started":"2026-01-07T11:22:39.040099Z","shell.execute_reply":"2026-01-07T11:22:39.077073Z"}},"outputs":[{"name":"stdout","text":"Saved baseline CSV -> /kaggle/working/semantic-llm-pruning/results/ablation_studies/baseline.csv\n                                               model input_model       tag  \\\n0  TinyLlama/TinyLlama-1.1B-intermediate-step-143...    baseline  baseline   \n\n   prune_ratio_heads  prune_ratio_mlp       ppl  latency_ms_per_token  \\\n0                0.0              0.0  7.382191             37.809968   \n\n       params       gpu    dtype            timestamp  \n0  1100048384  Tesla T4  float16  2026-01-07 11:22:39  \n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# --- Save baseline model for later reuse (Notebook 3 / 4) ---\n\nBASELINE_SAVE_PATH = \"/kaggle/working/semantic-llm-pruning/models/baseline\"\n\nos.makedirs(BASELINE_SAVE_PATH, exist_ok=True)\n\nmodel.save_pretrained(BASELINE_SAVE_PATH)\ntokenizer.save_pretrained(BASELINE_SAVE_PATH)\n\nprint(\"Baseline model saved to:\", BASELINE_SAVE_PATH)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T11:22:39.078455Z","iopub.execute_input":"2026-01-07T11:22:39.078690Z","iopub.status.idle":"2026-01-07T11:22:42.502326Z","shell.execute_reply.started":"2026-01-07T11:22:39.078653Z","shell.execute_reply":"2026-01-07T11:22:42.501564Z"}},"outputs":[{"name":"stdout","text":"Baseline model saved to: /kaggle/working/semantic-llm-pruning/models/baseline\n","output_type":"stream"}],"execution_count":16}]}