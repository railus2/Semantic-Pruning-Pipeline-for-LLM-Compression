{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":290428399,"sourceType":"kernelVersion"},{"sourceId":290436902,"sourceType":"kernelVersion"}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Paths IN/OUT + dossiers","metadata":{}},{"cell_type":"code","source":"import os\n\nBASE_IN  = \"/kaggle/input/02-structured-head-pruning-qo-gqa/semantic-llm-pruning\"   # <-- adapte si ton nom diffère\nBASE_OUT = \"/kaggle/working/semantic-llm-pruning\"\n\nART_IN = f\"{BASE_IN}/artifacts\"   # si tu as gardé artifacts dans l’output (sinon pointe vers notebook 01 output)\nMOD_IN = f\"{BASE_IN}/models\"\n\nos.makedirs(f\"{BASE_OUT}/models\", exist_ok=True)\nos.makedirs(f\"{BASE_OUT}/results/ablation_studies\", exist_ok=True)\n\nprint(\"BASE_IN:\", BASE_IN)\nprint(\"BASE_OUT:\", BASE_OUT)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-07T11:24:15.265707Z","iopub.execute_input":"2026-01-07T11:24:15.266034Z","iopub.status.idle":"2026-01-07T11:24:15.274832Z","shell.execute_reply.started":"2026-01-07T11:24:15.266010Z","shell.execute_reply":"2026-01-07T11:24:15.273986Z"}},"outputs":[{"name":"stdout","text":"BASE_IN: /kaggle/input/02-structured-head-pruning-qo-gqa/semantic-llm-pruning\nBASE_OUT: /kaggle/working/semantic-llm-pruning\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"Env “anti TF/JAX” (à mettre tout en haut)","metadata":{}},{"cell_type":"code","source":"import os\nos.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\nos.environ[\"TRANSFORMERS_NO_FLAX\"] = \"1\"\nos.environ[\"USE_TF\"] = \"0\"\nos.environ[\"USE_FLAX\"] = \"0\"\nos.environ[\"JAX_PLATFORM_NAME\"] = \"cpu\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T11:24:15.276189Z","iopub.execute_input":"2026-01-07T11:24:15.276492Z","iopub.status.idle":"2026-01-07T11:24:15.288968Z","shell.execute_reply.started":"2026-01-07T11:24:15.276466Z","shell.execute_reply":"2026-01-07T11:24:15.288418Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"Imports","metadata":{}},{"cell_type":"code","source":"import json\nimport numpy as np\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T11:24:15.289938Z","iopub.execute_input":"2026-01-07T11:24:15.290179Z","iopub.status.idle":"2026-01-07T11:24:25.575057Z","shell.execute_reply.started":"2026-01-07T11:24:15.290143Z","shell.execute_reply":"2026-01-07T11:24:25.574493Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"Charger le modèle d’entrée (celui pruné heads -20%)","metadata":{}},{"cell_type":"code","source":"PRUNED_HEADS_PATH = f\"{MOD_IN}/pruned_heads_20\"\n\ntokenizer = AutoTokenizer.from_pretrained(PRUNED_HEADS_PATH, use_fast=True)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    PRUNED_HEADS_PATH,\n).to(\"cuda\")\nmodel.eval()\n\nprint(\"Loaded:\", PRUNED_HEADS_PATH)\nprint(\"CUDA:\", torch.cuda.is_available(), torch.cuda.get_device_name(0))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T11:24:25.576766Z","iopub.execute_input":"2026-01-07T11:24:25.577184Z","iopub.status.idle":"2026-01-07T11:24:40.434852Z","shell.execute_reply.started":"2026-01-07T11:24:25.577161Z","shell.execute_reply":"2026-01-07T11:24:40.434194Z"}},"outputs":[{"name":"stdout","text":"Loaded: /kaggle/input/02-structured-head-pruning-qo-gqa/semantic-llm-pruning/models/pruned_heads_20\nCUDA: True Tesla T4\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"Charger l’importance MLP (depuis Notebook 01 output)","metadata":{}},{"cell_type":"code","source":"BASE_IN1 = \"/kaggle/input/00-setup-01-baseline-and-importance/semantic-llm-pruning\"\nmlp_importance = np.load(f\"{BASE_IN1}/artifacts/mlp_importance.npy\")\nprint(\"mlp_importance:\", mlp_importance.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T11:24:40.435610Z","iopub.execute_input":"2026-01-07T11:24:40.435859Z","iopub.status.idle":"2026-01-07T11:24:40.457160Z","shell.execute_reply.started":"2026-01-07T11:24:40.435836Z","shell.execute_reply":"2026-01-07T11:24:40.456623Z"}},"outputs":[{"name":"stdout","text":"mlp_importance: (22, 5632)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"Choisir les neurones MLP à garder (−10% et −20%)","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\nnum_layers = model.config.num_hidden_layers\nintermediate_size = mlp_importance.shape[1]\n\nassert mlp_importance.shape[0] == num_layers, (mlp_importance.shape, num_layers)\n\ndef build_mlp_keep_indices(mlp_importance, keep_ratio):\n    \"\"\"\n    keep_ratio: ex 0.90 pour -10%\n    return: dict layer -> np.array(indices gardés, triés)\n    \"\"\"\n    keep = {}\n    k = int(intermediate_size * keep_ratio)\n    k = max(1, k)\n\n    for l in range(num_layers):\n        scores = mlp_importance[l]  # [intermediate_size]\n        # top-k (scores élevés = importants)\n        idx = np.argpartition(scores, -k)[-k:]\n        idx = np.sort(idx)\n        keep[l] = idx.astype(np.int64)\n\n    return keep, k\n\nkeep_10, k10 = build_mlp_keep_indices(mlp_importance, keep_ratio=0.90)  # -10%\nkeep_20, k20 = build_mlp_keep_indices(mlp_importance, keep_ratio=0.80)  # -20%\n\nprint(\"intermediate_size:\", intermediate_size)\nprint(\"k10:\", k10, \"k20:\", k20)\nprint(\"layer0 sample idx (10%):\", keep_10[0][:10])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T11:24:40.458059Z","iopub.execute_input":"2026-01-07T11:24:40.458314Z","iopub.status.idle":"2026-01-07T11:24:40.474547Z","shell.execute_reply.started":"2026-01-07T11:24:40.458292Z","shell.execute_reply":"2026-01-07T11:24:40.473901Z"}},"outputs":[{"name":"stdout","text":"intermediate_size: 5632\nk10: 5068 k20: 4505\nlayer0 sample idx (10%): [ 1  2  3  4  5  6  7  8  9 10]\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"Pruner physiquement le MLP (gate/up/down)","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\ndef prune_out_features_like(linear: nn.Linear, keep_out_idx: torch.Tensor):\n    device = linear.weight.device\n    dtype  = linear.weight.dtype\n    new_linear = nn.Linear(\n        linear.in_features,\n        keep_out_idx.numel(),\n        bias=(linear.bias is not None),\n        device=device,\n        dtype=dtype,\n    )\n    new_linear.weight.data.copy_(linear.weight.data[keep_out_idx, :])\n    if linear.bias is not None:\n        new_linear.bias.data.copy_(linear.bias.data[keep_out_idx])\n    return new_linear\n\ndef prune_in_features_like(linear: nn.Linear, keep_in_idx: torch.Tensor):\n    device = linear.weight.device\n    dtype  = linear.weight.dtype\n    new_linear = nn.Linear(\n        keep_in_idx.numel(),\n        linear.out_features,\n        bias=(linear.bias is not None),\n        device=device,\n        dtype=dtype,\n    )\n    new_linear.weight.data.copy_(linear.weight.data[:, keep_in_idx])\n    if linear.bias is not None:\n        new_linear.bias.data.copy_(linear.bias.data)\n    return new_linear\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T11:24:40.475314Z","iopub.execute_input":"2026-01-07T11:24:40.475511Z","iopub.status.idle":"2026-01-07T11:24:40.482860Z","shell.execute_reply.started":"2026-01-07T11:24:40.475491Z","shell.execute_reply":"2026-01-07T11:24:40.482157Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"Fonction de pruning MLP sur un layer","metadata":{}},{"cell_type":"code","source":"def prune_mlp_layer(mlp, keep_idx_np):\n    \"\"\"\n    mlp: LlamaMLP\n    keep_idx_np: np.array indices (intermediate dim) à garder\n    \"\"\"\n    keep_idx = torch.tensor(keep_idx_np, dtype=torch.long, device=mlp.gate_proj.weight.device)\n\n    # gate_proj/up_proj: prune out_features\n    mlp.gate_proj = prune_out_features_like(mlp.gate_proj, keep_idx)\n    mlp.up_proj   = prune_out_features_like(mlp.up_proj, keep_idx)\n\n    # down_proj: prune in_features\n    mlp.down_proj = prune_in_features_like(mlp.down_proj, keep_idx)\n\n    return keep_idx.numel()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T11:24:40.483579Z","iopub.execute_input":"2026-01-07T11:24:40.483827Z","iopub.status.idle":"2026-01-07T11:24:40.495697Z","shell.execute_reply.started":"2026-01-07T11:24:40.483807Z","shell.execute_reply":"2026-01-07T11:24:40.495151Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"Créer et sauvegarder 2 modèles : MLP -10% et MLP -20%","metadata":{}},{"cell_type":"code","source":"import os\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ndef quick_generate(model, tokenizer, prompt=\"Explain MLP pruning.\", max_new_tokens=40):\n    device = next(model.parameters()).device\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n    out = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)\n    return tokenizer.decode(out[0], skip_special_tokens=True)\n\ndef prune_and_save_mlp(pruned_heads_path, keep_dict, tag, base_out):\n    # reload fresh (important)\n    tok = AutoTokenizer.from_pretrained(pruned_heads_path, use_fast=True)\n    if tok.pad_token is None:\n        tok.pad_token = tok.eos_token\n\n    m = AutoModelForCausalLM.from_pretrained(pruned_heads_path).to(\"cuda\").half().eval()\n\n    # apply pruning\n    for l in range(m.config.num_hidden_layers):\n        mlp = m.model.layers[l].mlp\n        new_int = prune_mlp_layer(mlp, keep_dict[l])\n\n    # patch config\n    m.config.intermediate_size = new_int\n\n    # sanity\n    txt = quick_generate(m, tok, prompt=f\"Give 2 bullet points about {tag}.\", max_new_tokens=40)\n    print(f\"[{tag}] generate ok:\\n\", txt[:300], \"\\n\")\n\n    # save\n    save_dir = f\"{base_out}/models/{tag}\"\n    os.makedirs(save_dir, exist_ok=True)\n    m.save_pretrained(save_dir)\n    tok.save_pretrained(save_dir)\n\n    print(\"Saved:\", save_dir)\n    return save_dir\n\n# run -10%\nsave_mlp10 = prune_and_save_mlp(PRUNED_HEADS_PATH, keep_10, \"pruned_heads20_mlp10\", BASE_OUT)\n\n# run -20%\nsave_mlp20 = prune_and_save_mlp(PRUNED_HEADS_PATH, keep_20, \"pruned_heads20_mlp20\", BASE_OUT)\n# =========================================================\n# (B) MLP pruning on BASELINE (normal model)\n# =========================================================\nBASELINE_MODEL_PATH = \"/kaggle/input/00-setup-01-baseline-and-importance/semantic-llm-pruning/models/baseline\"\n\n# IMPORTANT: on réutilise les mêmes keep_10 / keep_20\n# (Option \"comparaison apples-to-apples\")\nsave_base_mlp10 = prune_and_save_mlp(BASELINE_MODEL_PATH, keep_10, \"baseline_mlp10\", BASE_OUT)\nsave_base_mlp20 = prune_and_save_mlp(BASELINE_MODEL_PATH, keep_20, \"baseline_mlp20\", BASE_OUT)\n\nprint(\"Saved baseline_mlp10 ->\", save_base_mlp10)\nprint(\"Saved baseline_mlp20 ->\", save_base_mlp20)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T11:24:40.496518Z","iopub.execute_input":"2026-01-07T11:24:40.496801Z","iopub.status.idle":"2026-01-07T11:25:17.417070Z","shell.execute_reply.started":"2026-01-07T11:24:40.496771Z","shell.execute_reply":"2026-01-07T11:25:17.415988Z"}},"outputs":[{"name":"stdout","text":"[pruned_heads20_mlp10] generate ok:\n Give 2 bullet points about pruned_heads20_mlp10.\nThe first is that the head is a head of pruned heads.\nThe second is that the head is pruned.\nThe third is that the head is pruned. \n\nSaved: /kaggle/working/semantic-llm-pruning/models/pruned_heads20_mlp10\n[pruned_heads20_mlp20] generate ok:\n Give 2 bullet points about pruned_heads20_mlp20.\nThe 2018-2019 is the 2018-2019.\nThe 2018-2019 is the  \n\nSaved: /kaggle/working/semantic-llm-pruning/models/pruned_heads20_mlp20\n[baseline_mlp10] generate ok:\n Give 2 bullet points about baseline_mlp10.\n\n### 2.1.1. Baseline_mlp10\n\nBaseline_mlp10 is a baseline that uses the MLP10 model. \n\nSaved: /kaggle/working/semantic-llm-pruning/models/baseline_mlp10\n[baseline_mlp20] generate ok:\n Give 2 bullet points about baseline_mlp20.\n\n### 2.1.1. Baseline_mlp20\n\nBaseline_mlp20 is a 20-year baseline dataset. It is \n\nSaved: /kaggle/working/semantic-llm-pruning/models/baseline_mlp20\nSaved baseline_mlp10 -> /kaggle/working/semantic-llm-pruning/models/baseline_mlp10\nSaved baseline_mlp20 -> /kaggle/working/semantic-llm-pruning/models/baseline_mlp20\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"Vérifier que les dossiers existent","metadata":{}},{"cell_type":"code","source":"!ls -lah /kaggle/working/semantic-llm-pruning/models | head -n 50\n!ls -lah /kaggle/working/semantic-llm-pruning/models/pruned_heads20_mlp10 | head -n 30\n!ls -lah /kaggle/working/semantic-llm-pruning/models/pruned_heads20_mlp20 | head -n 30\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T11:25:17.420763Z","iopub.execute_input":"2026-01-07T11:25:17.421698Z","iopub.status.idle":"2026-01-07T11:25:18.098014Z","shell.execute_reply.started":"2026-01-07T11:25:17.421662Z","shell.execute_reply":"2026-01-07T11:25:18.096946Z"}},"outputs":[{"name":"stdout","text":"total 24K\ndrwxr-xr-x 6 root root 4.0K Jan  7 11:25 .\ndrwxr-xr-x 4 root root 4.0K Jan  7 11:24 ..\ndrwxr-xr-x 2 root root 4.0K Jan  7 11:25 baseline_mlp10\ndrwxr-xr-x 2 root root 4.0K Jan  7 11:25 baseline_mlp20\ndrwxr-xr-x 2 root root 4.0K Jan  7 11:24 pruned_heads20_mlp10\ndrwxr-xr-x 2 root root 4.0K Jan  7 11:24 pruned_heads20_mlp20\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"total 1.9G\ndrwxr-xr-x 2 root root 4.0K Jan  7 11:24 .\ndrwxr-xr-x 6 root root 4.0K Jan  7 11:25 ..\n-rw-r--r-- 1 root root  668 Jan  7 11:24 config.json\n-rw-r--r-- 1 root root  124 Jan  7 11:24 generation_config.json\n-rw-r--r-- 1 root root 1.9G Jan  7 11:24 model.safetensors\n-rw-r--r-- 1 root root  551 Jan  7 11:24 special_tokens_map.json\n-rw-r--r-- 1 root root  978 Jan  7 11:24 tokenizer_config.json\n-rw-r--r-- 1 root root 3.5M Jan  7 11:24 tokenizer.json\n-rw-r--r-- 1 root root 489K Jan  7 11:24 tokenizer.model\ntotal 1.7G\ndrwxr-xr-x 2 root root 4.0K Jan  7 11:24 .\ndrwxr-xr-x 6 root root 4.0K Jan  7 11:25 ..\n-rw-r--r-- 1 root root  668 Jan  7 11:24 config.json\n-rw-r--r-- 1 root root  124 Jan  7 11:24 generation_config.json\n-rw-r--r-- 1 root root 1.7G Jan  7 11:24 model.safetensors\n-rw-r--r-- 1 root root  551 Jan  7 11:24 special_tokens_map.json\n-rw-r--r-- 1 root root  978 Jan  7 11:24 tokenizer_config.json\n-rw-r--r-- 1 root root 3.5M Jan  7 11:24 tokenizer.json\n-rw-r--r-- 1 root root 489K Jan  7 11:24 tokenizer.model\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"import os, time, math, json\nimport pandas as pd\nimport torch\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\nBASE_OUT = \"/kaggle/working/semantic-llm-pruning\"\nOUT_CSV = f\"{BASE_OUT}/results/ablation_studies/mlp_pruning.csv\"\nos.makedirs(os.path.dirname(OUT_CSV), exist_ok=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T11:25:18.099623Z","iopub.execute_input":"2026-01-07T11:25:18.100035Z","iopub.status.idle":"2026-01-07T11:25:19.086658Z","shell.execute_reply.started":"2026-01-07T11:25:18.100005Z","shell.execute_reply":"2026-01-07T11:25:19.085928Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"@torch.no_grad()\ndef compute_ppl_wikitext2_zip_style(\n    model,\n    tokenizer,\n    split=\"test\",\n    seq_len=1024,\n    stride=512,\n    max_windows=256,\n    # alias Notebook-1:\n    num_samples=None,\n    max_length=None,\n):\n    if max_length is not None:\n        seq_len = int(max_length)\n    if num_samples is not None:\n        max_windows = int(num_samples)\n\n    model.eval()\n    ds = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=split)\n    text = \"\\n\\n\".join(ds[\"text\"])\n    enc = tokenizer(text, return_tensors=\"pt\")\n    input_ids = enc[\"input_ids\"][0]\n\n    device = next(model.parameters()).device\n    input_ids = input_ids.to(device)\n\n    nll_sum = 0.0\n    tok_count = 0\n    nb = 0\n\n    for start in range(0, input_ids.numel() - 1, stride):\n        end = min(start + seq_len, input_ids.numel())\n        x = input_ids[start:end].unsqueeze(0)\n\n        labels = x.clone()\n        if start > 0:\n            overlap = min(stride, labels.size(1))\n            labels[:, :-overlap] = -100\n\n        out = model(input_ids=x, labels=labels, use_cache=False, return_dict=True)\n        n_eval = (labels != -100).sum().item()\n\n        nll_sum += out.loss.detach().float().item() * max(n_eval, 1)\n        tok_count += n_eval\n\n        nb += 1\n        if nb >= max_windows or end == input_ids.numel():\n            break\n\n    ppl = float(torch.exp(torch.tensor(nll_sum / max(tok_count, 1))).item())\n    return ppl\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T11:25:19.087655Z","iopub.execute_input":"2026-01-07T11:25:19.088239Z","iopub.status.idle":"2026-01-07T11:25:19.877672Z","shell.execute_reply.started":"2026-01-07T11:25:19.088206Z","shell.execute_reply":"2026-01-07T11:25:19.876824Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"@torch.no_grad()\ndef measure_latency_ms_per_token(model, tokenizer, prompt=\"Explain MLP pruning.\", gen_tokens=64, runs=3):\n    model.eval()\n    device = next(model.parameters()).device\n    inp = tokenizer(prompt, return_tensors=\"pt\").to(device)\n\n    # warmup\n    _ = model.generate(**inp, max_new_tokens=gen_tokens, do_sample=False)\n\n    times = []\n    for _ in range(runs):\n        torch.cuda.synchronize()\n        t0 = time.time()\n        _ = model.generate(**inp, max_new_tokens=gen_tokens, do_sample=False)\n        torch.cuda.synchronize()\n        t1 = time.time()\n        times.append((t1 - t0) * 1000 / gen_tokens)\n\n    return sum(times) / len(times)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T11:25:19.878671Z","iopub.execute_input":"2026-01-07T11:25:19.879033Z","iopub.status.idle":"2026-01-07T11:25:33.304170Z","shell.execute_reply.started":"2026-01-07T11:25:19.879010Z","shell.execute_reply":"2026-01-07T11:25:33.303437Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"def count_params(model):\n    return sum(p.numel() for p in model.parameters())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T11:25:33.305051Z","iopub.execute_input":"2026-01-07T11:25:33.305340Z","iopub.status.idle":"2026-01-07T11:25:33.317657Z","shell.execute_reply.started":"2026-01-07T11:25:33.305307Z","shell.execute_reply":"2026-01-07T11:25:33.317049Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def eval_and_log_model(model_path, tag, prune_ratio_mlp, input_model_name):\n    tok = AutoTokenizer.from_pretrained(model_path, use_fast=True)\n    if tok.pad_token is None:\n        tok.pad_token = tok.eos_token\n\n    m = AutoModelForCausalLM.from_pretrained(model_path).to(\"cuda\").half().eval()\n\n    ppl = compute_ppl_wikitext2_zip_style(\n    m, tok,\n    split=\"test\",\n    seq_len=1024,\n    stride=512,\n    max_windows=256\n)\n    lat = measure_latency_ms_per_token(m, tok)\n    params = count_params(m)\n\n    row = {\n        \"model\": \"TinyLlama-1.1B\",\n        \"input_model\": input_model_name,\n        \"tag\": tag,\n        \"prune_ratio_mlp\": float(prune_ratio_mlp),\n        \"ppl\": float(ppl),\n        \"latency_ms_per_token\": float(lat),\n        \"params\": int(params),\n    }\n\n    df_new = pd.DataFrame([row])\n    if os.path.exists(OUT_CSV):\n        df_old = pd.read_csv(OUT_CSV)\n        df = pd.concat([df_old, df_new], ignore_index=True)\n    else:\n        df = df_new\n\n    df.to_csv(OUT_CSV, index=False)\n    print(f\"[{tag}] PPL={ppl:.3f} | latency={lat:.2f} ms/token | params={params:,}\")\n    print(\"Appended ->\", OUT_CSV)\n    return row\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T11:25:33.318438Z","iopub.execute_input":"2026-01-07T11:25:33.318678Z","iopub.status.idle":"2026-01-07T11:25:33.328786Z","shell.execute_reply.started":"2026-01-07T11:25:33.318658Z","shell.execute_reply":"2026-01-07T11:25:33.328128Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"BASE_MODELS = \"/kaggle/working/semantic-llm-pruning/models\"\n\n# (A) tes évaluations existantes (head+mlp) — tu les gardes\npath_mlp10 = f\"{BASE_MODELS}/pruned_heads20_mlp10\"\npath_mlp20 = f\"{BASE_MODELS}/pruned_heads20_mlp20\"\neval_and_log_model(path_mlp10, tag=\"pruned_heads20_mlp10\", prune_ratio_mlp=0.10, input_model_name=\"pruned_heads_20\")\neval_and_log_model(path_mlp20, tag=\"pruned_heads20_mlp20\", prune_ratio_mlp=0.20, input_model_name=\"pruned_heads_20\")\n\n# (B) AJOUT: baseline (normal) + baseline MLP-only\nbase_mlp10_path = f\"{BASE_MODELS}/baseline_mlp10\"\nbase_mlp20_path = f\"{BASE_MODELS}/baseline_mlp20\"\n\neval_and_log_model(base_mlp10_path, tag=\"baseline_mlp10\",  prune_ratio_mlp=0.10, input_model_name=\"baseline\")\neval_and_log_model(base_mlp20_path, tag=\"baseline_mlp20\",  prune_ratio_mlp=0.20, input_model_name=\"baseline\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T11:25:33.329493Z","iopub.execute_input":"2026-01-07T11:25:33.329757Z","iopub.status.idle":"2026-01-07T11:29:42.427181Z","shell.execute_reply.started":"2026-01-07T11:25:33.329709Z","shell.execute_reply":"2026-01-07T11:29:42.426543Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ebadc3058c674b918e8c8691df179f66"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"wikitext-2-raw-v1/test-00000-of-00001.pa(…):   0%|          | 0.00/733k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d37b6daa736c4e5b9b223972334f9d17"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"wikitext-2-raw-v1/train-00000-of-00001.p(…):   0%|          | 0.00/6.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2a563ca5446443e860ec9e364152e75"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"wikitext-2-raw-v1/validation-00000-of-00(…):   0%|          | 0.00/657k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4be8fb0f272b4f149e5c633832db1748"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"22def1d725af41cba33c7d09369f198c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d5b01621a724fdca8c8ee5d44e2238c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b5fbe3ac676040bfbf59d3d8f2689f25"}},"metadata":{}},{"name":"stdout","text":"[pruned_heads20_mlp10] PPL=15.424 | latency=27.09 ms/token | params=971,909,120\nAppended -> /kaggle/working/semantic-llm-pruning/results/ablation_studies/mlp_pruning.csv\n[pruned_heads20_mlp20] PPL=19.438 | latency=27.14 ms/token | params=895,809,536\nAppended -> /kaggle/working/semantic-llm-pruning/results/ablation_studies/mlp_pruning.csv\n[baseline_mlp10] PPL=8.020 | latency=26.96 ms/token | params=1,023,813,632\nAppended -> /kaggle/working/semantic-llm-pruning/results/ablation_studies/mlp_pruning.csv\n[baseline_mlp20] PPL=9.194 | latency=27.05 ms/token | params=947,714,048\nAppended -> /kaggle/working/semantic-llm-pruning/results/ablation_studies/mlp_pruning.csv\n","output_type":"stream"},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"{'model': 'TinyLlama-1.1B',\n 'input_model': 'baseline',\n 'tag': 'baseline_mlp20',\n 'prune_ratio_mlp': 0.2,\n 'ppl': 9.194290161132812,\n 'latency_ms_per_token': 27.04836552341779,\n 'params': 947714048}"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"!cat /kaggle/working/semantic-llm-pruning/results/ablation_studies/mlp_pruning.csv\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T11:29:42.428105Z","iopub.execute_input":"2026-01-07T11:29:42.428394Z","iopub.status.idle":"2026-01-07T11:29:42.638444Z","shell.execute_reply.started":"2026-01-07T11:29:42.428369Z","shell.execute_reply":"2026-01-07T11:29:42.637698Z"}},"outputs":[{"name":"stdout","text":"model,input_model,tag,prune_ratio_mlp,ppl,latency_ms_per_token,params\nTinyLlama-1.1B,pruned_heads_20,pruned_heads20_mlp10,0.1,15.424039840698242,27.08899850646655,971909120\nTinyLlama-1.1B,pruned_heads_20,pruned_heads20_mlp20,0.2,19.438156127929688,27.138307690620422,895809536\nTinyLlama-1.1B,baseline,baseline_mlp10,0.1,8.019798278808594,26.96259195605914,1023813632\nTinyLlama-1.1B,baseline,baseline_mlp20,0.2,9.194290161132812,27.04836552341779,947714048\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":17}]}