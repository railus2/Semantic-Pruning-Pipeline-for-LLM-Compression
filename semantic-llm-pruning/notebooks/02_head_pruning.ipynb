{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":290428399,"sourceType":"kernelVersion"}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!ls /kaggle/input/00-setup-01-baseline-and-importance\n!ls /kaggle/input/00-setup-01-baseline-and-importance/semantic-llm-pruning\n!ls /kaggle/input/00-setup-01-baseline-and-importance/semantic-llm-pruning/artifacts\n!ls /kaggle/input/00-setup-01-baseline-and-importance/semantic-llm-pruning/results\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-07T11:12:51.185189Z","iopub.execute_input":"2026-01-07T11:12:51.185884Z","iopub.status.idle":"2026-01-07T11:12:51.686213Z","shell.execute_reply.started":"2026-01-07T11:12:51.185853Z","shell.execute_reply":"2026-01-07T11:12:51.685520Z"}},"outputs":[{"name":"stdout","text":"custom.css\t    __output__.json   semantic-llm-pruning\n__notebook__.ipynb  __results__.html\nartifacts  models  results\nhead_importance.npy  head_ranking.json\tmlp_importance.npy  mlp_ranking.json\nablation_studies  latency_baseline.json  perplexity_baseline.json\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"Charger les rankings","metadata":{}},{"cell_type":"code","source":"BASE_IN  = \"/kaggle/input/00-setup-01-baseline-and-importance/semantic-llm-pruning\"\nBASE_OUT = \"/kaggle/working/semantic-llm-pruning\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T11:12:51.687943Z","iopub.execute_input":"2026-01-07T11:12:51.688319Z","iopub.status.idle":"2026-01-07T11:12:51.692704Z","shell.execute_reply.started":"2026-01-07T11:12:51.688287Z","shell.execute_reply":"2026-01-07T11:12:51.692100Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import json, numpy as np\n\nwith open(f\"{BASE_IN}/artifacts/head_ranking.json\") as f:\n    head_rank = json.load(f)\n\nmlp_importance = np.load(f\"{BASE_IN}/artifacts/mlp_importance.npy\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T11:12:51.693632Z","iopub.execute_input":"2026-01-07T11:12:51.693851Z","iopub.status.idle":"2026-01-07T11:12:51.780095Z","shell.execute_reply.started":"2026-01-07T11:12:51.693833Z","shell.execute_reply":"2026-01-07T11:12:51.779538Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"0) Paths (IN/OUT) + dossiers","metadata":{}},{"cell_type":"code","source":"import os, json\nimport numpy as np\nimport torch\n\nBASE_IN  = \"/kaggle/input/00-setup-01-baseline-and-importance/semantic-llm-pruning\"\nBASE_OUT = \"/kaggle/working/semantic-llm-pruning\"\n\nos.makedirs(f\"{BASE_OUT}/models\", exist_ok=True)\nos.makedirs(f\"{BASE_OUT}/results/ablation_studies\", exist_ok=True)\n\nART_IN = f\"{BASE_IN}/artifacts\"\nprint(\"IN artifacts:\", ART_IN)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T11:12:51.781502Z","iopub.execute_input":"2026-01-07T11:12:51.781715Z","iopub.status.idle":"2026-01-07T11:12:58.982762Z","shell.execute_reply.started":"2026-01-07T11:12:51.781696Z","shell.execute_reply":"2026-01-07T11:12:58.982157Z"}},"outputs":[{"name":"stdout","text":"IN artifacts: /kaggle/input/00-setup-01-baseline-and-importance/semantic-llm-pruning/artifacts\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"1) Charger le ranking des heads (Notebook 01 output)","metadata":{}},{"cell_type":"code","source":"with open(f\"{ART_IN}/head_ranking.json\", \"r\") as f:\n    head_rank = json.load(f)\n\nnum_layers = head_rank[\"num_layers\"]\nnum_heads  = head_rank[\"num_heads\"]\nranking    = head_rank[\"ranking\"]\n\nprint(\"Layers:\", num_layers, \"Heads/layer:\", num_heads, \"Ranking entries:\", len(ranking))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T11:12:58.983646Z","iopub.execute_input":"2026-01-07T11:12:58.984063Z","iopub.status.idle":"2026-01-07T11:12:58.991392Z","shell.execute_reply.started":"2026-01-07T11:12:58.984029Z","shell.execute_reply":"2026-01-07T11:12:58.990604Z"}},"outputs":[{"name":"stdout","text":"Layers: 22 Heads/layer: 32 Ranking entries: 704\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"2) Construire la liste des heads à garder (top 80% par layer)","metadata":{}},{"cell_type":"markdown","source":"3) Recharger le modèle de base (obligatoire)","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\n\nMODEL_ID = \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\"\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_ID,\n    dtype=torch.float16,\n).to(\"cuda\")\n\nmodel.eval()\n\n\nprint(\"Model device:\", next(model.parameters()).device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T11:12:58.992177Z","iopub.execute_input":"2026-01-07T11:12:58.993377Z","iopub.status.idle":"2026-01-07T11:13:51.107699Z","shell.execute_reply.started":"2026-01-07T11:12:58.993342Z","shell.execute_reply":"2026-01-07T11:13:51.106656Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/776 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed3a8b22566b4cc9b2d13d913bc05464"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"739c8e9b937644cc94959d1f8c796ca9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4a1ecb5cf434cd2a70255f823b1aaea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0e15f8875b54456be25e5542513975d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/560 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"805c1b9db63a4248a354277686095e8e"}},"metadata":{}},{"name":"stderr","text":"2026-01-07 11:13:17.133920: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1767784397.590984      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1767784397.697394      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1767784398.727723      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767784398.727766      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767784398.727769      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767784398.727772      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/4.40G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2b90f126d464b58afdee8f0da15616b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/129 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ce0c4b10c504f7ab502f3f1ef6651ac"}},"metadata":{}},{"name":"stdout","text":"Model device: cuda:0\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"bad = []\nfor name, p in model.named_parameters():\n    if p.device.type != \"cuda\":\n        bad.append((name, str(p.device)))\nprint(\"Non-cuda params:\", bad[:5], \"count:\", len(bad))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T11:13:51.108933Z","iopub.execute_input":"2026-01-07T11:13:51.109615Z","iopub.status.idle":"2026-01-07T11:13:51.117024Z","shell.execute_reply.started":"2026-01-07T11:13:51.109587Z","shell.execute_reply":"2026-01-07T11:13:51.116224Z"}},"outputs":[{"name":"stdout","text":"Non-cuda params: [] count: 0\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"✅ Étape 4 (refaite proprement) — Head pruning -20% Q+O seulement, compatible GQA + même device/dtype","metadata":{}},{"cell_type":"markdown","source":"4.0 Préparer old_q_heads et head_dim","metadata":{}},{"cell_type":"code","source":"num_layers = model.config.num_hidden_layers\n\nold_q_heads = model.config.num_attention_heads\nkv_heads = getattr(model.config, \"num_key_value_heads\", None)\n\nhead_dim = getattr(model.config, \"head_dim\", None)\nif head_dim is None:\n    attn0 = model.model.layers[0].self_attn\n    head_dim = attn0.q_proj.out_features // old_q_heads\n\nprint(\"old_q_heads:\", old_q_heads, \"| kv_heads:\", kv_heads, \"| head_dim:\", head_dim)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T11:13:51.118081Z","iopub.execute_input":"2026-01-07T11:13:51.118408Z","iopub.status.idle":"2026-01-07T11:14:03.637288Z","shell.execute_reply.started":"2026-01-07T11:13:51.118369Z","shell.execute_reply":"2026-01-07T11:14:03.636500Z"}},"outputs":[{"name":"stdout","text":"old_q_heads: 32 | kv_heads: 4 | head_dim: 64\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"PRUNE_RATIO = 0.20\n\ntarget = int(old_q_heads * (1.0 - PRUNE_RATIO))\n\nif kv_heads is not None and kv_heads > 0:\n    target = target - (target % kv_heads)\n    target = max(target, kv_heads)\nelse:\n    target = max(target, 1)\n\nKEEP_PER_LAYER = target\nprint(\"KEEP_PER_LAYER (valid):\", KEEP_PER_LAYER)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T11:14:03.638377Z","iopub.execute_input":"2026-01-07T11:14:03.638687Z","iopub.status.idle":"2026-01-07T11:14:03.659785Z","shell.execute_reply.started":"2026-01-07T11:14:03.638654Z","shell.execute_reply":"2026-01-07T11:14:03.659154Z"}},"outputs":[{"name":"stdout","text":"KEEP_PER_LAYER (valid): 24\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# --- build per-layer q_scores from `ranking` (list of dicts: layer/head/score) ---\nold_q_heads = model.config.num_attention_heads\nold_kv_heads = getattr(model.config, \"num_key_value_heads\", None)\nassert old_kv_heads is not None, \"Ce patch suppose un modèle GQA (num_key_value_heads existe).\"\n\n# objectif: tu veux garder KEEP_PER_LAYER têtes Q par couche (comme avant)\nNEW_Q_HEADS = KEEP_PER_LAYER\n\n# propose un pruning KV proportionnel, puis ajuste pour que NEW_Q_HEADS % NEW_KV_HEADS == 0\nNEW_KV_HEADS = max(1, int(round(old_kv_heads * (NEW_Q_HEADS / old_q_heads))))\nwhile NEW_KV_HEADS > 1 and (NEW_Q_HEADS % NEW_KV_HEADS != 0):\n    NEW_KV_HEADS -= 1\n\nNEW_N_REP = NEW_Q_HEADS // NEW_KV_HEADS\nprint(\"Plan:\", \"old_q=\", old_q_heads, \"old_kv=\", old_kv_heads, \"| new_q=\", NEW_Q_HEADS, \"new_kv=\", NEW_KV_HEADS, \"new_n_rep=\", NEW_N_REP)\n\n# q_scores[l][h] = score\nq_scores = [[0.0 for _ in range(old_q_heads)] for _ in range(num_layers)]\nfor item in ranking:\n    l = item[\"layer\"]\n    h = item[\"head\"]\n    s = float(item.get(\"score\", item.get(\"importance\", 0.0)))\n    q_scores[l][h] = s\n\ngroup_size = old_q_heads // old_kv_heads\n\nkv_keep_heads = {}\nq_keep_heads = {}\n\nfor l in range(num_layers):\n    # 1) score KV = moyenne des scores Q du groupe KV\n    kv_scores = []\n    for g in range(old_kv_heads):\n        start = g * group_size\n        end = (g + 1) * group_size\n        kv_scores.append((g, sum(q_scores[l][start:end]) / max(1, (end - start))))\n\n    kv_scores.sort(key=lambda x: x[1], reverse=True)\n    kv_keep = sorted([g for g, _ in kv_scores[:NEW_KV_HEADS]])\n    kv_keep_heads[l] = kv_keep\n\n    # 2) pour chaque KV gardé, garder NEW_N_REP têtes Q DANS ce groupe\n    q_keep = []\n    for g in kv_keep:\n        start = g * group_size\n        end = (g + 1) * group_size\n        heads = list(range(start, end))\n        heads.sort(key=lambda h: q_scores[l][h], reverse=True)\n        q_keep.extend(heads[:NEW_N_REP])\n\n    q_keep = sorted(q_keep)\n    q_keep_heads[l] = q_keep\n\nprint(\"Example layer0 kv_keep:\", kv_keep_heads[0])\nprint(\"Example layer0 q_keep count:\", len(q_keep_heads[0]), \"heads:\", q_keep_heads[0])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T11:14:03.663173Z","iopub.execute_input":"2026-01-07T11:14:03.663532Z","iopub.status.idle":"2026-01-07T11:14:03.683559Z","shell.execute_reply.started":"2026-01-07T11:14:03.663500Z","shell.execute_reply":"2026-01-07T11:14:03.683008Z"}},"outputs":[{"name":"stdout","text":"Plan: old_q= 32 old_kv= 4 | new_q= 24 new_kv= 3 new_n_rep= 8\nExample layer0 kv_keep: [0, 1, 2]\nExample layer0 q_keep count: 24 heads: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"4.1 Helpers “like” (même device/dtype)","metadata":{}},{"cell_type":"markdown","source":"4.2 Fonction pruning Q+O (GQA-safe)","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\ndef _prune_linear_out(linear: nn.Linear, out_idx: torch.Tensor) -> nn.Linear:\n    W = linear.weight.data.index_select(0, out_idx).contiguous()\n    b = linear.bias.data.index_select(0, out_idx).contiguous() if linear.bias is not None else None\n    new = nn.Linear(linear.in_features, out_idx.numel(), bias=(b is not None)).to(W.device, W.dtype)\n    new.weight.data.copy_(W)\n    if b is not None:\n        new.bias.data.copy_(b)\n    return new\n\ndef _prune_linear_in(linear: nn.Linear, in_idx: torch.Tensor) -> nn.Linear:\n    W = linear.weight.data.index_select(1, in_idx).contiguous()\n    b = linear.bias.data.contiguous() if linear.bias is not None else None\n    new = nn.Linear(in_idx.numel(), linear.out_features, bias=(b is not None)).to(W.device, W.dtype)\n    new.weight.data.copy_(W)\n    if b is not None:\n        new.bias.data.copy_(b)\n    return new\n\ndef _head_to_dim_indices(head_ids, head_dim: int, device):\n    idx = []\n    for h in head_ids:\n        base = h * head_dim\n        idx.extend(range(base, base + head_dim))\n    return torch.tensor(idx, device=device, dtype=torch.long)\n\n@torch.no_grad()\ndef prune_gqa_qkvo(attn, q_keep_heads, kv_keep_heads, head_dim: int):\n    device = attn.q_proj.weight.device\n    q_out_idx  = _head_to_dim_indices(q_keep_heads, head_dim, device)\n    kv_out_idx = _head_to_dim_indices(kv_keep_heads, head_dim, device)\n\n    # prune OUT\n    attn.q_proj = _prune_linear_out(attn.q_proj, q_out_idx)\n    attn.k_proj = _prune_linear_out(attn.k_proj, kv_out_idx)\n    attn.v_proj = _prune_linear_out(attn.v_proj, kv_out_idx)\n\n    # prune IN\n    attn.o_proj = _prune_linear_in(attn.o_proj, q_out_idx)\n\n    # patch attributs éventuels\n    for attr in [\"num_heads\", \"num_attention_heads\"]:\n        if hasattr(attn, attr):\n            setattr(attn, attr, len(q_keep_heads))\n    if hasattr(attn, \"head_dim\"):\n        attn.head_dim = head_dim\n\n    return len(q_keep_heads), len(kv_keep_heads)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T11:14:03.685621Z","iopub.execute_input":"2026-01-07T11:14:03.685876Z","iopub.status.idle":"2026-01-07T11:14:03.711308Z","shell.execute_reply.started":"2026-01-07T11:14:03.685854Z","shell.execute_reply":"2026-01-07T11:14:03.710427Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"4.3 Appliquer à tous les layers","metadata":{}},{"cell_type":"code","source":"new_q_list = []\nnew_kv_list = []\n\nfor l in range(num_layers):\n    attn = model.model.layers[l].self_attn\n    new_q, new_kv = prune_gqa_qkvo(attn, q_keep_heads[l], kv_keep_heads[l], head_dim)\n    new_q_list.append(new_q)\n    new_kv_list.append(new_kv)\n\nprint(\"New q-heads (layer0):\", new_q_list[0])\nprint(\"New kv-heads (layer0):\", new_kv_list[0])\nprint(\"All layers same q?\", len(set(new_q_list)) == 1)\nprint(\"All layers same kv?\", len(set(new_kv_list)) == 1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T11:14:03.712677Z","iopub.execute_input":"2026-01-07T11:14:03.713176Z","iopub.status.idle":"2026-01-07T11:14:04.812251Z","shell.execute_reply.started":"2026-01-07T11:14:03.713141Z","shell.execute_reply":"2026-01-07T11:14:04.811444Z"}},"outputs":[{"name":"stdout","text":"New q-heads (layer0): 24\nNew kv-heads (layer0): 3\nAll layers same q? True\nAll layers same kv? True\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"attn = model.model.layers[0].self_attn\nprint(\"q_heads =\", model.config.num_attention_heads)\nprint(\"kv_heads =\", getattr(model.config, \"num_key_value_heads\", None))\nprint(\"q_proj.out =\", attn.q_proj.out_features)\nprint(\"k_proj.out =\", attn.k_proj.out_features)\nprint(\"head_dim =\", attn.q_proj.out_features // model.config.num_attention_heads)\nprint(\"n_rep =\", model.config.num_attention_heads // getattr(model.config, \"num_key_value_heads\", 1))\nprint(\"divisible?\", model.config.num_attention_heads % getattr(model.config, \"num_key_value_heads\", 1) == 0)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T11:14:04.813328Z","iopub.execute_input":"2026-01-07T11:14:04.813609Z","iopub.status.idle":"2026-01-07T11:14:04.819362Z","shell.execute_reply.started":"2026-01-07T11:14:04.813585Z","shell.execute_reply":"2026-01-07T11:14:04.818768Z"}},"outputs":[{"name":"stdout","text":"q_heads = 32\nkv_heads = 4\nq_proj.out = 1536\nk_proj.out = 192\nhead_dim = 48\nn_rep = 8\ndivisible? True\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"attn0 = model.model.layers[0].self_attn\nprint(\"q_out:\", attn0.q_proj.out_features)\nprint(\"k_out:\", attn0.k_proj.out_features)\nprint(\"v_out:\", attn0.v_proj.out_features)\nprint(\"o_in :\", attn0.o_proj.in_features)\nprint(\"devices:\",\n      attn0.q_proj.weight.device,\n      attn0.k_proj.weight.device,\n      attn0.o_proj.weight.device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T11:14:04.820132Z","iopub.execute_input":"2026-01-07T11:14:04.820380Z","iopub.status.idle":"2026-01-07T11:14:04.839749Z","shell.execute_reply.started":"2026-01-07T11:14:04.820358Z","shell.execute_reply":"2026-01-07T11:14:04.839240Z"}},"outputs":[{"name":"stdout","text":"q_out: 1536\nk_out: 192\nv_out: 192\no_in : 1536\ndevices: cuda:0 cuda:0 cuda:0\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"5) Mettre à jour la config (important pour save/load)","metadata":{}},{"cell_type":"code","source":"model.config.num_attention_heads = NEW_Q_HEADS\nsetattr(model.config, \"num_key_value_heads\", NEW_KV_HEADS)\nsetattr(model.config, \"head_dim\", head_dim)\n\nprint(\"Config:\",\n      \"q_heads =\", model.config.num_attention_heads,\n      \"kv_heads =\", getattr(model.config, \"num_key_value_heads\", None),\n      \"head_dim =\", model.config.head_dim,\n      \"n_rep =\", model.config.num_attention_heads // getattr(model.config, \"num_key_value_heads\", 1),\n      \"divisible =\", (model.config.num_attention_heads % getattr(model.config, \"num_key_value_heads\", 1) == 0))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T11:14:04.840533Z","iopub.execute_input":"2026-01-07T11:14:04.840724Z","iopub.status.idle":"2026-01-07T11:14:04.858324Z","shell.execute_reply.started":"2026-01-07T11:14:04.840704Z","shell.execute_reply":"2026-01-07T11:14:04.857460Z"}},"outputs":[{"name":"stdout","text":"Config: q_heads = 24 kv_heads = 3 head_dim = 64 n_rep = 8 divisible = True\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"6) Sanity check rapide (forward + génération)","metadata":{}},{"cell_type":"code","source":"@torch.no_grad()\ndef quick_generate(model, tokenizer, prompt=\"Explain attention pruning.\", max_new_tokens=40):\n    device = next(model.parameters()).device\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n    out = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)\n    return tokenizer.decode(out[0], skip_special_tokens=True)\n\nprint(quick_generate(model, tokenizer))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T11:14:04.859401Z","iopub.execute_input":"2026-01-07T11:14:04.859722Z","iopub.status.idle":"2026-01-07T11:14:07.230165Z","shell.execute_reply.started":"2026-01-07T11:14:04.859672Z","shell.execute_reply":"2026-01-07T11:14:07.229459Z"}},"outputs":[{"name":"stdout","text":"Explain attention pruning.\n\n    # Explain why we use attention.\n    # Explain why we use attention.\n    # Explain why we use attention.\n    # Explain why we use attention\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"7) Sauvegarder le modèle pruné (Notebook 02 output)","metadata":{}},{"cell_type":"code","source":"import os\n\nsave_dir = f\"{BASE_OUT}/models/pruned_heads_20\"\nos.makedirs(save_dir, exist_ok=True)\n\nmodel.save_pretrained(save_dir)\ntokenizer.save_pretrained(save_dir)\n\nprint(\"Saved pruned model to:\", save_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T11:14:07.231116Z","iopub.execute_input":"2026-01-07T11:14:07.231444Z","iopub.status.idle":"2026-01-07T11:14:10.534405Z","shell.execute_reply.started":"2026-01-07T11:14:07.231407Z","shell.execute_reply":"2026-01-07T11:14:10.533716Z"}},"outputs":[{"name":"stdout","text":"Saved pruned model to: /kaggle/working/semantic-llm-pruning/models/pruned_heads_20\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\n\ntmp_tok = AutoTokenizer.from_pretrained(save_dir, use_fast=True)\ntmp_model = AutoModelForCausalLM.from_pretrained(save_dir, dtype=torch.float16).to(\"cuda\").eval()\n\nprint(quick_generate(tmp_model, tmp_tok, prompt=\"Give 2 bullet points about pruning.\", max_new_tokens=40))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T11:14:10.535334Z","iopub.execute_input":"2026-01-07T11:14:10.535642Z","iopub.status.idle":"2026-01-07T11:14:12.468058Z","shell.execute_reply.started":"2026-01-07T11:14:10.535613Z","shell.execute_reply":"2026-01-07T11:14:12.467262Z"}},"outputs":[{"name":"stdout","text":"Give 2 bullet points about pruning.\nI'm not sure if this is a good idea or not.\nI'm not sure if this is a good idea.\nI'm not sure if this is a good idea\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"import torch, math\nfrom datasets import load_dataset\n\n@torch.no_grad()\ndef compute_ppl_wikitext2_zip_style(\n    model,\n    tokenizer,\n    split=\"test\",\n    seq_len=1024,\n    stride=512,\n    max_windows=256,\n    num_samples=None,\n    max_length=None,\n):\n    if max_length is not None:\n        seq_len = int(max_length)\n    if num_samples is not None:\n        max_windows = int(num_samples)\n\n    model.eval()\n    ds = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=split)\n    text = \"\\n\\n\".join(ds[\"text\"])\n    enc = tokenizer(text, return_tensors=\"pt\")\n    input_ids = enc[\"input_ids\"][0]\n\n    device = next(model.parameters()).device\n    input_ids = input_ids.to(device)\n\n    nll_sum = 0.0\n    tok_count = 0\n    nb = 0\n\n    for start in range(0, input_ids.numel() - 1, stride):\n        end = min(start + seq_len, input_ids.numel())\n        x = input_ids[start:end].unsqueeze(0)\n\n        labels = x.clone()\n        if start > 0:\n            overlap = min(stride, labels.size(1))\n            labels[:, :-overlap] = -100\n\n        out = model(input_ids=x, labels=labels, use_cache=False, return_dict=True)\n        n_eval = (labels != -100).sum().item()\n\n        nll_sum += out.loss.detach().float().item() * max(n_eval, 1)\n        tok_count += n_eval\n\n        nb += 1\n        if nb >= max_windows or end == input_ids.numel():\n            break\n\n    ppl = float(torch.exp(torch.tensor(nll_sum / max(tok_count, 1))).item())\n    return ppl\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T11:14:12.469180Z","iopub.execute_input":"2026-01-07T11:14:12.469589Z","iopub.status.idle":"2026-01-07T11:14:13.513575Z","shell.execute_reply.started":"2026-01-07T11:14:12.469561Z","shell.execute_reply":"2026-01-07T11:14:13.512976Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"import time\n\n@torch.no_grad()\ndef measure_latency(model, tokenizer, prompt=\"Explain attention pruning.\", gen_tokens=64, runs=3):\n    device = next(model.parameters()).device\n    inp = tokenizer(prompt, return_tensors=\"pt\").to(device)\n\n    # warmup\n    _ = model.generate(**inp, max_new_tokens=gen_tokens)\n\n    times = []\n    for _ in range(runs):\n        torch.cuda.synchronize()\n        t0 = time.time()\n        _ = model.generate(**inp, max_new_tokens=gen_tokens)\n        torch.cuda.synchronize()\n        t1 = time.time()\n        times.append((t1 - t0) * 1000 / gen_tokens)\n\n    return sum(times) / len(times)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T11:14:13.514479Z","iopub.execute_input":"2026-01-07T11:14:13.515038Z","iopub.status.idle":"2026-01-07T11:14:13.520085Z","shell.execute_reply.started":"2026-01-07T11:14:13.515012Z","shell.execute_reply":"2026-01-07T11:14:13.519527Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"def count_params(model):\n    return sum(p.numel() for p in model.parameters())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T11:14:13.520773Z","iopub.execute_input":"2026-01-07T11:14:13.521026Z","iopub.status.idle":"2026-01-07T11:14:13.549140Z","shell.execute_reply.started":"2026-01-07T11:14:13.520991Z","shell.execute_reply":"2026-01-07T11:14:13.548231Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"import pandas as pd\nimport os\n\nppl = compute_ppl_wikitext2_zip_style(model, tokenizer, num_samples=200, max_length=512)\nlat_ms = measure_latency(model, tokenizer)\nparams = count_params(model)\n\nprint(\"PPL:\", ppl)\nprint(\"Latency (ms/token):\", lat_ms)\nprint(\"Params:\", params)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T11:14:13.550113Z","iopub.execute_input":"2026-01-07T11:14:13.550347Z","iopub.status.idle":"2026-01-07T11:14:39.404671Z","shell.execute_reply.started":"2026-01-07T11:14:13.550324Z","shell.execute_reply":"2026-01-07T11:14:39.403925Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8631c1d0e067436e82a3bb4ee710df00"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"wikitext-2-raw-v1/test-00000-of-00001.pa(…):   0%|          | 0.00/733k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8bbabc6baef44c809ee765268767bb87"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"wikitext-2-raw-v1/train-00000-of-00001.p(…):   0%|          | 0.00/6.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d8fcc96a02434c5a81e4a27a7217cfd1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"wikitext-2-raw-v1/validation-00000-of-00(…):   0%|          | 0.00/657k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f26ec4c4f95c42148393fa81a76bd6a9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9c6c815f72c4592b07c4279637685d5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4efbc771e13d464ab5845573e6fc1cda"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e78b7394dca14016b2e0de097539929b"}},"metadata":{}},{"name":"stdout","text":"PPL: 17.405532836914062\nLatency (ms/token): 27.193083117405575\nParams: 1048143872\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"OUT_CSV = f\"{BASE_OUT}/results/ablation_studies/head_pruning.csv\"\nos.makedirs(os.path.dirname(OUT_CSV), exist_ok=True)\n\nrow = {\n    \"model\": \"TinyLlama-1.1B\",\n    \"pruning_type\": \"head_qo\",\n    \"prune_ratio\": 0.20,\n    \"num_attention_heads\": model.config.num_attention_heads,\n    \"num_key_value_heads\": getattr(model.config, \"num_key_value_heads\", None),\n    \"params\": params,\n    \"perplexity_wikitext2\": ppl,\n    \"latency_ms_per_token\": lat_ms,\n}\n\ndf_new = pd.DataFrame([row])\n\nif os.path.exists(OUT_CSV):\n    df_old = pd.read_csv(OUT_CSV)\n    df = pd.concat([df_old, df_new], ignore_index=True)\nelse:\n    df = df_new\n\ndf.to_csv(OUT_CSV, index=False)\nprint(\"Saved metrics to:\", OUT_CSV)\ndf\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T11:14:39.405560Z","iopub.execute_input":"2026-01-07T11:14:39.405845Z","iopub.status.idle":"2026-01-07T11:14:39.453008Z","shell.execute_reply.started":"2026-01-07T11:14:39.405808Z","shell.execute_reply":"2026-01-07T11:14:39.452187Z"}},"outputs":[{"name":"stdout","text":"Saved metrics to: /kaggle/working/semantic-llm-pruning/results/ablation_studies/head_pruning.csv\n","output_type":"stream"},{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"            model pruning_type  prune_ratio  num_attention_heads  \\\n0  TinyLlama-1.1B      head_qo          0.2                   24   \n\n   num_key_value_heads      params  perplexity_wikitext2  latency_ms_per_token  \n0                    3  1048143872             17.405533             27.193083  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>model</th>\n      <th>pruning_type</th>\n      <th>prune_ratio</th>\n      <th>num_attention_heads</th>\n      <th>num_key_value_heads</th>\n      <th>params</th>\n      <th>perplexity_wikitext2</th>\n      <th>latency_ms_per_token</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>TinyLlama-1.1B</td>\n      <td>head_qo</td>\n      <td>0.2</td>\n      <td>24</td>\n      <td>3</td>\n      <td>1048143872</td>\n      <td>17.405533</td>\n      <td>27.193083</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"!cat /kaggle/working/semantic-llm-pruning/results/ablation_studies/head_pruning.csv\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T11:14:39.454095Z","iopub.execute_input":"2026-01-07T11:14:39.454450Z","iopub.status.idle":"2026-01-07T11:14:39.652685Z","shell.execute_reply.started":"2026-01-07T11:14:39.454423Z","shell.execute_reply":"2026-01-07T11:14:39.651913Z"}},"outputs":[{"name":"stdout","text":"model,pruning_type,prune_ratio,num_attention_heads,num_key_value_heads,params,perplexity_wikitext2,latency_ms_per_token\nTinyLlama-1.1B,head_qo,0.2,24,3,1048143872,17.405532836914062,27.193083117405575\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":24}]}